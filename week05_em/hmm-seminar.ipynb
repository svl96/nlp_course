{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    move_out_count = np.zeros(num_states)\n",
    "\n",
    "    for n, sequence in enumerate(sequences):\n",
    "        for prev, nxt in zip(sequence[:-1], sequence[1:]):\n",
    "            A_counts[prev, nxt] += 1\n",
    "            pi_counts[prev] += 1\n",
    "        if len(sequence) > 0:\n",
    "            pi_counts[sequence[-1]] += 1\n",
    "            \n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, axis=1)[:, None]\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9], [20, 8, 5, 0, 16, 5, 18, 9, 15, 4]]\n",
      "CPU times: user 5.51 s, sys: 40 ms, total: 5.55 s\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "print(plaintext_sequences[:2])\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 µs, sys: 0 ns, total: 45 µs\n",
      "Wall time: 47.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((27,),\n",
       " (27, 27),\n",
       " 1.0,\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pi.shape, A.shape, pi.sum(), A.sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping) # number of states = number of chars \n",
    "        self.num_outputs = len(observations_to_char_mapping) # number of outputs ?\n",
    "        self.states_to_char_mapping = states_to_char_mapping # get char from state\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping # what is observations ??? \n",
    "       \n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)# init pi with random \n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states).astype('float128') # init transition matrix with random \n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs).astype('float128') # init emission matrix \n",
    "        self.B /= np.sum(self.B, 1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        self.pi = parameters['pi'] if self.fixed_pi else self.pi \n",
    "        \n",
    "        self.fixed_A = 'A' in parameters\n",
    "        self.A = parameters['A'] if self.fixed_A else self.A\n",
    "        \n",
    "        self.fixed_B = 'B' in parameters\n",
    "        self.B = parameters['B'] if self.fixed_B else self.B\n",
    "           \n",
    "        previous_llh = None # prev log-likelihood\n",
    "        \n",
    "        iteration = 0\n",
    "        while True and iteration < max_iters: # why we dont use for-loop here? \n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences) # E-step from EM-algo\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts) # M-step from EM-algo\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            if iteration % 5 == 0:\n",
    "                self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            print('iteration %d; log likelihood %.4f' % (iteration, log_likelihood))\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iteration += 1\n",
    "        self.output(sequences[:10])\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            # Run Forward-Backward dynamic program\n",
    "            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sequence)\n",
    "  \n",
    "            # Accumulate statistics.\n",
    "            pi_counts += gamma[:, 0]\n",
    "            A_counts += xi\n",
    "            for t, x in enumerate(sequence):\n",
    "                B_counts[:, x] += gamma[:, t]\n",
    "            \n",
    "            total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence) \n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        ss = np.sum(alpha * beta, 0, dtype=np.float128)\n",
    "        if (ss == 0).any():\n",
    "            print('err')\n",
    "        assert np.isfinite(ss).all()\n",
    "        gamma = (alpha * beta) / ss\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A)\n",
    "            this_xi += (self.A * alpha[:, t][:, None]) * (beta[:, t+1] * self.B[:, sequence[t+1]])\n",
    "\n",
    "#             for i in range(self.num_states):\n",
    "#                 this_xi[i, :] += alpha[i, t] * self.A[i, :] * beta[:, t+1] * self.B[:, sequence[t+1]]\n",
    "#                 for j in range(self.num_states):\n",
    "#                     this_xi[i, j] += alpha[i, t] * self.A[i, j] * beta[j, t+1] * self.B[j, sequence[t+1]]        \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)), dtype=np.float128)\n",
    "#         print(alpha.shape)\n",
    "        alpha[:, 0] = self.pi * self.B[:, sequence[0]]\n",
    "        for t in range(len(sequence) - 1):\n",
    "            x = sequence[t+1]\n",
    "            \n",
    "#             alpha[:, t+1] = self.B[:, x] * np.sum(self.A * alpha[:, t][:, None], axis=0) \n",
    "            alpha[:, t+1] = self.B[:, x] * np.sum(self.A * alpha[:, t][:, None], axis=0, dtype=np.float128)\n",
    "#             for i in range(self.num_states):\n",
    "# #                 sum_val = sum([alpha[j, t] * self.A[j, i] for j in range(self.num_states)])\n",
    "#                 sum_val = np.sum(self.A[:, i] * alpha[:, t], dtype=np.float128)\n",
    "# #                 print(alpha.dtype, A.dtype)\n",
    "# #                 assert sum_val == floatsum_sec), \"{} {}\".format(sum_val, sum_sec)\n",
    "#                 alpha[i, t+1] = self.B[i, x] * sum_val\n",
    "\n",
    "        return alpha \n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        beta[:, -1] = 1\n",
    "        \n",
    "#         for i in range(self.num_states):\n",
    "#             beta[i, -1] = 1\n",
    "        \n",
    "        for i, x in enumerate(sequence[1:][::-1]):\n",
    "            t = len(sequence) - 1 - i\n",
    "#             beta[:, t-1] = np.sum(self.A * (beta[:, t] * self.B[:, x])[:, None], axis=1) # check \n",
    "\n",
    "            beta[:, t-1] = np.sum(self.A * (beta[:, t] * self.B[:, x]), axis=1, dtype=np.float128)\n",
    "            \n",
    "#             for i in range(self.num_states):\n",
    "#                 beta[i, t-1] = sum([beta[j, t] * self.A[i, j] * self.B[j, x] for j in range(self.num_states)]) \n",
    "\n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        clear_output()\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = [self.observations_to_char_mapping[x] for x in sequence]                \n",
    "            map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sequence)]\n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.],\n",
       "       [3.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mt = np.ones((2,3))\n",
    "np.sum(mt, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(states):       than such a roman\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       cassous brutus bait not me\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       ill not endure it you forget yourself\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       to hedge me in i am a soldier i\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       older in prackice abler than yourself\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       to mave conditions\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       brutus go to you are not cassous\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       cassous i am\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       brutus i say you are not\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       cassous inge me no more i thall forget myself\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:100], parameters={'pi': pi, 'A': A})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float128"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
