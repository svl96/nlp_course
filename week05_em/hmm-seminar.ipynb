{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    move_out_count = np.zeros(num_states)\n",
    "\n",
    "    for n, sequence in enumerate(sequences):\n",
    "        for prev, nxt in zip(sequence[:-1], sequence[1:]):\n",
    "            A_counts[prev, nxt] += 1\n",
    "            pi_counts[prev] += 1\n",
    "        if len(sequence) > 0:\n",
    "            pi_counts[sequence[-1]] += 1\n",
    "            \n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, axis=1)[:, None]\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.68 s, sys: 24 ms, total: 5.71 s\n",
      "Wall time: 5.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}, max_len=60):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping) # number of states = number of chars \n",
    "        self.num_outputs = len(observations_to_char_mapping) # number of outputs ?\n",
    "        self.states_to_char_mapping = states_to_char_mapping # get char from state\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping # what is observations ??? \n",
    "        \n",
    "        self.max_len = max_len ## MAX LEN OF ENCRIPTED TEXT\n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)# init pi with random \n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states).astype('float128') # init transition matrix with random \n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs).astype('float128') # init emission matrix \n",
    "        self.B /= np.sum(self.B, 1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        self.pi = parameters['pi'] if self.fixed_pi else self.pi \n",
    "        \n",
    "        self.fixed_A = 'A' in parameters\n",
    "        self.A = parameters['A'] if self.fixed_A else self.A\n",
    "        \n",
    "        self.fixed_B = 'B' in parameters\n",
    "        self.B = parameters['B'] if self.fixed_B else self.B\n",
    "           \n",
    "        previous_llh = None # prev log-likelihood\n",
    "        \n",
    "        iteration = 0\n",
    "        while True and iteration < max_iters: # why we dont use for-loop here? \n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences) # E-step from EM-algo\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts) # M-step from EM-algo\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            if iteration % 10 == 0:\n",
    "                # clear_output()\n",
    "                print('iteration %d; log likelihood %.4f' % (iteration, log_likelihood))\n",
    "                # self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iteration += 1\n",
    "        return previous_llh, self.output(sequences[:10])\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "\n",
    "                # Run Forward-Backward dynamic program\n",
    "                alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sub_seq)\n",
    "\n",
    "                # Accumulate statistics.\n",
    "                pi_counts += gamma[:, 0]\n",
    "                A_counts += xi\n",
    "                for t, x in enumerate(sub_seq):\n",
    "                    B_counts[:, x] += gamma[:, t]\n",
    "\n",
    "                total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence) \n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        ss = np.sum(alpha * beta, 0, dtype=np.float128)\n",
    "        if (ss == 0).any():\n",
    "            print('err')\n",
    "        assert np.isfinite(ss).all()\n",
    "        gamma = (alpha * beta) / ss\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A) ## Changed for better performance \n",
    "            this_xi += (self.A * alpha[:, t][:, None]) * (beta[:, t+1] * self.B[:, sequence[t+1]])      \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)), dtype=np.float128)\n",
    "        alpha[:, 0] = self.pi * self.B[:, sequence[0]]\n",
    "        for t in range(len(sequence) - 1):\n",
    "            x = sequence[t+1]\n",
    "            alpha[:, t+1] = self.B[:, x] * np.sum(self.A * alpha[:, t][:, None], axis=0, dtype=np.float128)\n",
    "        return alpha \n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        beta[:, -1] = 1\n",
    "        \n",
    "        for i, x in enumerate(sequence[1:][::-1]):\n",
    "            t = len(sequence) - 1 - i\n",
    "            beta[:, t-1] = np.sum(self.A * (beta[:, t] * self.B[:, x]), axis=1, dtype=np.float128)\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        res = []\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = []\n",
    "            map_states = []\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "                sub_observations = [self.observations_to_char_mapping[x] for x in sub_seq]                \n",
    "                sub_map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sub_seq)]\n",
    "                observations.append(''.join(sub_observations))\n",
    "                map_states.append(''.join(sub_map_states))\n",
    "                \n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "            res.append('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -12078.0470\n",
      "iteration 10; log likelihood -9866.1163\n",
      "iteration 20; log likelihood -9360.5749\n",
      "iteration 30; log likelihood -8951.6571\n",
      "iteration 40; log likelihood -8733.8902\n",
      "iteration 50; log likelihood -8687.9314\n",
      "iteration 60; log likelihood -8680.0509\n",
      "iteration 70; log likelihood -8675.7187\n",
      "iteration 80; log likelihood -8670.7223\n",
      "iteration 90; log likelihood -8666.3393\n",
      "(states):       than such a roman\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       cassous brutus bait not me\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       ill not endure it you fonget yourself\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       to hedge me in i am a soldier i\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       older in prackice abler than yourself\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       to mave conditions\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       brutus go to you are not cassius\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       cassous i am\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       brutus i say you are not\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       cassous inge me no more i thall fonget myself\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:100], parameters={'pi': pi, 'A': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 results \n",
    "We can see that hmm works good enough with small english texts\n",
    "\n",
    "p.s. this is work of algorithm with task3-4 improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -22996.2079\n",
      "iteration 10; log likelihood -18070.3598\n",
      "iteration 20; log likelihood -16542.6550\n",
      "iteration 30; log likelihood -15735.2477\n",
      "iteration 40; log likelihood -15635.3415\n",
      "(states):       князь андрей пожал плечами и поморщился как морщатся любители музыки услышав пальшивую ноту обе женщины отпустили друг друга потом опять как бучто боясь опоздать схватили друг друга за руки стали целовать и отрывать руки и потом опять стали целовать друг друга в лицо и совершенно неожиданно для князя андрея обе заплакали и опять стали целоваться тоже 1921919ла князю андрею было очевидно неловко но для двух женщин казалось так естественно что они плакали казалось они и не предполагали чтобы могло иначе совершиться это свидание\n",
      "(observations): йдрьуопдшщмзояю4пфояфм3пч о ояючющ8 ф0ройпйочющ8пъ0рофцх ъмф очнь2й он0ф2гпвоипфуг внцодюъноюхмо4мд8 д2оюъян0ъ ф ошщн1ошщн1пояюъючоюяръуойпйохншъюохюр0уоюяюьшпъуо0твпъ ф ошщн1ошщн1поьпощнй о0ъпф о5мфювпъуо оюъщ2впъуощнй о ояюъючоюяръуо0ъпф о5мфювпъуошщн1ошщн1повоф 5юо о0ювмщгмддюодмю4 шпддюошфройдрьропдшщмроюхмоьпяфпйпф о оюяръуо0ъпф о5мфювпъу0роъю4моьпяфпйпфпойдрьцопдшщмцох2фюою3мв шдюодмфювйюодюошфрошвнто4мд8 дойпьпфю0уоъпйом0ъм0ъвмддюо3ъюоюд ояфпйпф ойпьпфю0уоюд о одмоящмшяюфп1пф о3ъюх2очю1фюо дп3мо0ювмщг ъу0ро9ъюо0в шпд м\n",
      "(states):       вдруг заговорили обе женщины и засмеялись с ах милая ах мари а я видела во сне так вы нас не ожидали ах мари вы так похудели а вы так пополнел\n",
      "(observations): вшщн1оьп1ювющ ф оюхмо4мд8 д2о оьп0чмрф 0уо0опточ фпропточпщ опоров шмфповюо0дмоъпйов2одп0одмою4 шпф опточпщ ов2оъпйояютншмф опов2оъпйояюяюфдмф\n",
      "(states):       я тотчас узнала княщиню вставила бурьен\n",
      "(observations): роъюъ3п0оньдпфпойдр1 дцов0ъпв фпохнщумд\n",
      "(states):       восклицала княжна марья а я не подозревала ах я и не видела тебя\n",
      "(observations): вю0йф 5пфпойдр4дпочпщуропородмояюшюьщмвпфпопторо одмов шмфпоъмхр\n",
      "(states):       князь андрей поцеловался с сестрою рука в руку и сказал ей что она такая же плак7а как всегда была княжна марья повернулась к брату и сквозь слезы любовный теплый и кроткий взгляд ее прекрасных в ту минуту больших лучистых глаз остановился на лице князя андрея\n",
      "(observations): йдрьуопдшщмзояю5мфювпф0ро0о0м0ъщюцощнйповощнйно о0йпьпфомзо3ъюоюдпоъпйпро4мояфпй0пойпйов0м1шпох2фпойдр4дпочпщурояювмщднфп0уойохщпъно о0йвюьуо0фмь2офцхювд2зоъмяф2зо ойщюъй зовь1фршоммоящмйщп0д2товоъноч днънохюфуг тофн3 0ъ2то1фпьою0ъпдюв ф0родпоф 5мойдрьропдшщмр\n",
      "(states):       княщиня говорила без умолку короткая верхняя губка с усиками то и дело на мгновение слетала вниз притрогивалась где нужно было к румяной нижней губке и вновь открывалась блестевшая зубами и глазами улыбка княщиня рассказывала случай который был с ними на спасской горе грозивший ей опасностию в ее положении и сейчас же после этого стобщила что она все платья свои оставила в петербурге и здесь будет ходить бог знает в чем и что андрей совсем переменился и что китти одынцова вышла замуж за старика и что есть жених для княжны марьи вполне серьезный но что об этом поговорим после княжна марья все еще молча смотрела на брата и в прекрасных глазах ее была и любовь и грусть видно было что в ней установился теперь свой ход мысли независимый от речей невестки она в середине ее рассказа о последнем празднике в петербурге обратилась к брату\n",
      "(observations): йдр1 дро1ювющ фпохмьончюфйнойющюъйпровмщтдрро1нхйпо0он0 йпч оъюо ошмфюодпоч1дювмд мо0фмъпфповд ьоящ ъщю1 впфп0уо1шмодн4дюох2фюойощнчрдюзод 4дмзо1нхймо овдювуоюъйщ2впфп0уохфм0ъмвгпроьнхпч о о1фпьпч онф2хйпойдр1 дрощп00йпь2впфпо0фн3пзойюъющ2зох2фо0од ч одпо0яп00йюзо1ющмо1щюь вг зомзоюяп0дю0ъ цовоммояюфю4мд  о о0мз3п0о4мояю0фмо9ъю1юо0ююх8 фпо3ъюоюдпов0мояфпъуро0вю ою0ъпв фповоямъмщхнщ1мо оьшм0уохншмъотюш ъуохю1оьдпмъово3мчо о3ъюопдшщмзо0юв0мчоямщмчмд ф0ро о3ъюой ъъ оюш2д5ювпов2гфпоьпчн4оьпо0ъпщ йпо о3ъюом0ъуо4мд тошфройдр4д2очпщу овяюфдмо0мщумьд2зодюо3ъюоюхо9ъючояю1ювющ чояю0фмойдр4дпочпщуров0мом8мочюф3по0чюъщмфподпохщпъпо овоящмйщп0д2то1фпьптоммох2фпо офцхювуо о1щн0ъуов шдюох2фюо3ъюоводмзон0ъпдюв ф0роъмямщуо0вюзотюшоч20ф одмьпв 0 ч2зоюъощм3мзодмвм0ъй оюдпово0мщмш дмоммощп00йпьпоюояю0фмшдмчоящпьшд ймовоямъмщхнщ1моюхщпъ фп0уойохщпън\n",
      "(states):        ты решительно едешь на войну сказала вздохнув\n",
      "(observations): оъ2ощмг ъмфудюомшмгуодповюздно0йпьпфповьшютднв\n",
      "(states):       вздрогнула тоже\n",
      "(observations): вьшщю1днфпоъю4м\n",
      "(states):       даже завтра отвечал брат\n",
      "(observations): шп4моьпвъщпоюъвм3пфохщпъ\n",
      "(states):       ой покидает меня здесь и бог знает зачем тогда как ой мог бы получить повышение\n",
      "(observations): юдояюй шпмъочмдроьшм0уо охю1оьдпмъоьп3мчоъю1шпойпйоюдочю1ох2ояюфн3 ъуояюв2гмд м\n"
     ]
    }
   ],
   "source": [
    "plaintext_russian = 'plaintext/russian.txt'\n",
    "ciphertext_russian = 'encrypted/99_encrypted.txt'\n",
    "\n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext_russian)\n",
    "\n",
    "plaintext_sequences = load_sequences(plaintext_russian, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext_russian, char_to_int_mapping)\n",
    "pi_ru, A_ru = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n",
    "\n",
    "\n",
    "\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, max_len=15)\n",
    "\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:30], parameters={'pi': pi_ru, 'A': A_ru}, max_iters=50)\n",
    "# take first 30 encripted examples to reduce time of processing\n",
    "# for the same reason change the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2 Results\n",
    "- Hmm works quite good for that section, but we needed to change "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian():\n",
    "    russian_list = []\n",
    "    for i in range(143):\n",
    "        try:\n",
    "            ciphertext_find = 'encrypted/{}_encrypted.txt'.format(i)\n",
    "            encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "            russian_list.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    return russian_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 98,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 104,\n",
       " 106,\n",
       " 109,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 124,\n",
       " 126,\n",
       " 127,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 136,\n",
       " 138,\n",
       " 141,\n",
       " 142]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = get_russian()\n",
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_find = 'plaintext/english.txt'\n",
    "char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plaintext_find)\n",
    "plaintext_sequences_tt = load_sequences(plaintext_find, char_to_int_mapping_tt)\n",
    "pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ciphertext_find = 'encrypted/26_encrypted.txt' # longer sequences in russian\n",
    "encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "def find_min(files, cipher_file):\n",
    "    scores = []\n",
    "    for plain_file in files:\n",
    "        print(plain_file)\n",
    "        char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plain_file)\n",
    "        plaintext_sequences_tt = load_sequences(plain_file, char_to_int_mapping_tt)\n",
    "        \n",
    "        ciphertext_find = cipher_file # longer sequences in russian\n",
    "        encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "        \n",
    "        pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))\n",
    "        # Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "        state_to_char = int_to_char_mapping_tt\n",
    "        observation_to_char = int_to_char_mapping_tt\n",
    "\n",
    "        # Initialize a HMM with the correct state/output spaces.\n",
    "        hmm_tt = HMM(observation_to_char, state_to_char)\n",
    "\n",
    "        # Estimate the parameters and decode the encrypted sequences.\n",
    "        llh, res = hmm_tt.estimate_with_em(encrypted_sequences_tt[:100], parameters={'pi': pi_tt, 'A': A_tt}, max_iters=60)\n",
    "        scores.append((llh, res))\n",
    "        clear_output()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shec  142\n",
      "['(states):       the heart of brothers govern in our loves\\n(observations): noqxoqehnxwpxkhwnoqhjxvwzqhixgixwthxdwzqj', '(states):       and sway our great designs\\n(observations): ei xjmebxwthxvhqenx qjgvij', '(states):       ckesar there is my han\\n(observations): ceqjehxnoqhqxgjxfbxoei', '(states):       a sister i bequeath you whom no brother\\n(observations): exjgjnqhxgxkqrtqenoxbwtxmowfxiwxkhwnoqh', '(states):       id iver love so dearly let her live\\n(observations): g xqzqhxdwzqxjwx qehdbxdqnxoqhxdgzq', '(states):       to join our kingdomy and our hearts and never\\n(observations): nwxlwgixwthxagiv wfjxei xwthxoqehnjxei xiqzqh', '(states):       fly off our loves again\\n(observations): pdbxwppxwthxdwzqjxevegi', '(states):       lexidus happily amen\\n(observations): dqyg tjxoeyygdbxefqi', '(states):       antond i did not thind to draw my sword gainst pompey\\n(observations): einwibxgx g xiwnxnogiaxnwx hemxfbxjmwh xvegijnxywfyqb', '(states):       for he hath laid sthange courtesies and great\\n(observations): pwhxoqxoenoxdeg xjnheivqxcwthnqjgqjxei xvhqen']\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'plaintext/dickens.txt',\n",
    "    'plaintext/shakespeare.txt'\n",
    "]\n",
    "\n",
    "rr = []\n",
    "for num in rl:\n",
    "    res = find_min(files, 'encrypted/{}_encrypted.txt'.format(num))\n",
    "    \n",
    "    if res[0][0] > res[1][0]:\n",
    "        print('dick ', num)\n",
    "        print(res[0][1])\n",
    "        rr.append([('dick', num), (res[0][1])])\n",
    "    else:\n",
    "        print('shec ', num)\n",
    "        print(res[1][1])\n",
    "        rr.append([('shec', num), (res[1][1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results', 'wb') as f:\n",
    "    pickle.dump(rr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for rs in rr:\n",
    "#     print(rs[0])\n",
    "    if rs[0][0] == 'shec':\n",
    "        data.append('encrypted/{}_encrypted.txt '.format(rs[0][1]) +  'plaintext/shakespeare.txt\\n')\n",
    "    else:\n",
    "        data.append('encrypted/{}_encrypted.txt '.format(rs[0][1]) +  'plaintext/dickens.txt\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_auth.txt', 'w', encoding='utf8') as f:\n",
    "    f.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
