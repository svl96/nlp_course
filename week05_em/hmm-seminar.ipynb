{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    move_out_count = np.zeros(num_states)\n",
    "\n",
    "    for n, sequence in enumerate(sequences):\n",
    "        for prev, nxt in zip(sequence[:-1], sequence[1:]):\n",
    "            A_counts[prev, nxt] += 1\n",
    "            pi_counts[prev] += 1\n",
    "        if len(sequence) > 0:\n",
    "            pi_counts[sequence[-1]] += 1\n",
    "            \n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, axis=1)[:, None]\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.61 s, sys: 12 ms, total: 5.62 s\n",
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}, max_len=15):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping) # number of states = number of chars \n",
    "        self.num_outputs = len(observations_to_char_mapping) # number of outputs ?\n",
    "        self.states_to_char_mapping = states_to_char_mapping # get char from state\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping # what is observations ??? \n",
    "        \n",
    "        self.max_len = max_len ## MAX LEN OF ENCRIPTED TEXT\n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)# init pi with random \n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states) # init transition matrix with random \n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs) # init emission matrix \n",
    "        self.B /= np.sum(self.B, 1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        self.pi = parameters['pi'] if self.fixed_pi else self.pi \n",
    "        \n",
    "        self.fixed_A = 'A' in parameters\n",
    "        self.A = parameters['A'] if self.fixed_A else self.A\n",
    "        \n",
    "        self.fixed_B = 'B' in parameters\n",
    "        self.B = parameters['B'] if self.fixed_B else self.B\n",
    "           \n",
    "        previous_llh = None # prev log-likelihood\n",
    "        \n",
    "        iteration = 0\n",
    "        while True and iteration < max_iters: # why we dont use for-loop here? \n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences) # E-step from EM-algo\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts) # M-step from EM-algo\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            if iteration % 10 == 0:\n",
    "                # clear_output()\n",
    "                print('iteration %d; log likelihood %.4f' % (iteration, log_likelihood))\n",
    "                # self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iteration += 1\n",
    "        return previous_llh, self.output(sequences[:10])\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "\n",
    "                # Run Forward-Backward dynamic program\n",
    "                alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sub_seq)\n",
    "\n",
    "                # Accumulate statistics.\n",
    "                pi_counts += gamma[:, 0]\n",
    "                A_counts += xi\n",
    "                for t, x in enumerate(sub_seq):\n",
    "                    B_counts[:, x] += gamma[:, t]\n",
    "\n",
    "                total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence) \n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        ss = np.sum(alpha * beta, 0)\n",
    "        if (ss == 0).any():\n",
    "            print('err')\n",
    "        assert np.isfinite(ss).all()\n",
    "        gamma = (alpha * beta) / ss\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A) ## Changed for better performance \n",
    "            this_xi += (self.A * alpha[:, t][:, None]) * (beta[:, t+1] * self.B[:, sequence[t+1]])      \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        alpha[:, 0] = self.pi * self.B[:, sequence[0]]\n",
    "        for t in range(len(sequence) - 1):\n",
    "            x = sequence[t+1]\n",
    "            alpha[:, t+1] = self.B[:, x] * np.sum(self.A * alpha[:, t][:, None], axis=0)\n",
    "        return alpha \n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        beta[:, -1] = 1\n",
    "        \n",
    "        for i, x in enumerate(sequence[1:][::-1]):\n",
    "            t = len(sequence) - 1 - i\n",
    "            beta[:, t-1] = np.sum(self.A * (beta[:, t] * self.B[:, x]), axis=1)\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        res = []\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = []\n",
    "            map_states = []\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "                sub_observations = [self.observations_to_char_mapping[x] for x in sub_seq]                \n",
    "                sub_map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sub_seq)]\n",
    "                observations.append(''.join(sub_observations))\n",
    "                map_states.append(''.join(sub_map_states))\n",
    "                \n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "            res.append('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def output_decripted(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        res = []\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = []\n",
    "            map_states = []\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "                sub_map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sub_seq)]\n",
    "                map_states.append(''.join(sub_map_states))\n",
    "            res.append(''.join(map_states) + '\\n')\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -12169.9094\n",
      "iteration 10; log likelihood -10090.3446\n",
      "iteration 20; log likelihood -9539.3666\n",
      "iteration 30; log likelihood -9356.4698\n",
      "iteration 40; log likelihood -9104.9786\n",
      "iteration 50; log likelihood -8830.2878\n",
      "iteration 60; log likelihood -8781.2650\n",
      "iteration 70; log likelihood -8755.8358\n",
      "iteration 80; log likelihood -8748.2929\n",
      "iteration 90; log likelihood -8743.7812\n",
      "(states):       than such a roman\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       caslous brutus bait not me\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       ild not endure it you forget yoursenf\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       to hedge me in i am a sondier i\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       onder in practice abler than yoursenf\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       to mave conditions\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       brutus go to you are not caslius\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       caslous i am\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       brutus i say you are not\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       caslous urge me no more i thall forget mysenf\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, max_len=15)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:100], parameters={'pi': pi, 'A': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 results \n",
    "We can see that hmm works good enough with small english texts\n",
    "\n",
    "p.s. this is work of algorithm with task3-4 improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -37535.9083\n",
      "iteration 10; log likelihood -26762.8259\n",
      "iteration 20; log likelihood -25593.4310\n",
      "iteration 30; log likelihood -25131.5963\n",
      "iteration 40; log likelihood -25084.5277\n",
      "iteration 50; log likelihood -25073.6968\n",
      "(states):       919191914086 пожал плечами и поморщился как морщатся любители музыки услышав чальшивую ноту обе женщины отпустили друг друга потом опять как будто боясь опоздать схватили друг друга за руки стали целовать и отрывать руки и потом опять стали целовать друг друга в лицо и совершенно неожиданно для княз91914087 обе заплакали и опять стали целоваться тоже заплакала 519191914087 было очевидно неловко но для двух женщий казалось так естественно что они плакали казалось они и не предполагали чтобы могло иначе совершиться это свидание\n",
      "(observations): йдрьуопдшщмзояю4пфояфм3пч о ояючющ8 ф0ройпйочющ8пъ0рофцх ъмф очнь2й он0ф2гпвоипфуг внцодюъноюхмо4мд8 д2оюъян0ъ ф ошщн1ошщн1пояюъючоюяръуойпйохншъюохюр0уоюяюьшпъуо0твпъ ф ошщн1ошщн1поьпощнй о0ъпф о5мфювпъуо оюъщ2впъуощнй о ояюъючоюяръуо0ъпф о5мфювпъуошщн1ошщн1повоф 5юо о0ювмщгмддюодмю4 шпддюошфройдрьропдшщмроюхмоьпяфпйпф о оюяръуо0ъпф о5мфювпъу0роъю4моьпяфпйпфпойдрьцопдшщмцох2фюою3мв шдюодмфювйюодюошфрошвнто4мд8 дойпьпфю0уоъпйом0ъм0ъвмддюо3ъюоюд ояфпйпф ойпьпфю0уоюд о одмоящмшяюфп1пф о3ъюх2очю1фюо дп3мо0ювмщг ъу0ро9ъюо0в шпд м\n",
      "(states):       вдруг заговорили обе женщины и засмеялись с ах милая ах мари а я видела во сне так вы нас не ожидали ах мари вы так помудели а вы так пополнел\n",
      "(observations): вшщн1оьп1ювющ ф оюхмо4мд8 д2о оьп0чмрф 0уо0опточ фпропточпщ опоров шмфповюо0дмоъпйов2одп0одмою4 шпф опточпщ ов2оъпйояютншмф опов2оъпйояюяюфдмф\n",
      "(states):       я тотчас узнала княщиню вставила бурьен\n",
      "(observations): роъюъ3п0оньдпфпойдр1 дцов0ъпв фпохнщумд\n",
      "(states):       восклицала княжна марья а я не подозревала ах я и не видела тебя\n",
      "(observations): вю0йф 5пфпойдр4дпочпщуропородмояюшюьщмвпфпопторо одмов шмфпоъмхр\n",
      "(states):       919191914086 поцеловался с сестрою рука в руку и сказал ей что она такая же плакса как всегда была княжна марья повернулась к брату и сквозь слезы любовный теплый и кроткий взгляд ее прекрасных в ту минуту больших лучистых глаз остановился на лице к19191914089\n",
      "(observations): йдрьуопдшщмзояю5мфювпф0ро0о0м0ъщюцощнйповощнйно о0йпьпфомзо3ъюоюдпоъпйпро4мояфпй0пойпйов0м1шпох2фпойдр4дпочпщурояювмщднфп0уойохщпъно о0йвюьуо0фмь2офцхювд2зоъмяф2зо ойщюъй зовь1фршоммоящмйщп0д2товоъноч днънохюфуг тофн3 0ъ2то1фпьою0ъпдюв ф0родпоф 5мойдрьропдшщмр\n",
      "(states):       княщиня говорила без умолку короткая верхняя губка с усиками то и дело на мгновение слетала вниз притрогивалась где нужно было к румяной нижней губке и вновь открывалась блестевшая зубами и глазами улыбка княщиня рассказывала случай который был с ними на спасской горе грозивший ей опасностию в ее положении и сейчас же после этого сообщила что она все платья свои оставила в петербующе и здесь будет ходить бог знает в чем и что андрей совсем переменился и что китти одынцова вышла замуж за старика и что есть жених для княжны марьи вполне серьезный но что об этом поговорим после княжна марья все еще молча смотрела на брата и в прекрасных глазах ее была и любовь и грусть видно было что в ней установился теперь свой ход мысли независимый от речей невестки она в середине ее рассказа о последнем празднике в петербующе обратилась к брату\n",
      "(observations): йдр1 дро1ювющ фпохмьончюфйнойющюъйпровмщтдрро1нхйпо0он0 йпч оъюо ошмфюодпоч1дювмд мо0фмъпфповд ьоящ ъщю1 впфп0уо1шмодн4дюох2фюойощнчрдюзод 4дмзо1нхймо овдювуоюъйщ2впфп0уохфм0ъмвгпроьнхпч о о1фпьпч онф2хйпойдр1 дрощп00йпь2впфпо0фн3пзойюъющ2зох2фо0од ч одпо0яп00йюзо1ющмо1щюь вг зомзоюяп0дю0ъ цовоммояюфю4мд  о о0мз3п0о4мояю0фмо9ъю1юо0ююх8 фпо3ъюоюдпов0мояфпъуро0вю ою0ъпв фповоямъмщхнщ1мо оьшм0уохншмъотюш ъуохю1оьдпмъово3мчо о3ъюопдшщмзо0юв0мчоямщмчмд ф0ро о3ъюой ъъ оюш2д5ювпов2гфпоьпчн4оьпо0ъпщ йпо о3ъюом0ъуо4мд тошфройдр4д2очпщу овяюфдмо0мщумьд2зодюо3ъюоюхо9ъючояю1ювющ чояю0фмойдр4дпочпщуров0мом8мочюф3по0чюъщмфподпохщпъпо овоящмйщп0д2то1фпьптоммох2фпо офцхювуо о1щн0ъуов шдюох2фюо3ъюоводмзон0ъпдюв ф0роъмямщуо0вюзотюшоч20ф одмьпв 0 ч2зоюъощм3мзодмвм0ъй оюдпово0мщмш дмоммощп00йпьпоюояю0фмшдмчоящпьшд ймовоямъмщхнщ1моюхщпъ фп0уойохщпън\n",
      "(states):        ты решительно едешь на войну сказала вздохнув\n",
      "(observations): оъ2ощмг ъмфудюомшмгуодповюздно0йпьпфповьшютднв\n",
      "(states):       вздрогнула тоже\n",
      "(observations): вьшщю1днфпоъю4м\n",
      "(states):       даже завтра отвечал брат\n",
      "(observations): шп4моьпвъщпоюъвм3пфохщпъ\n",
      "(states):       ой покидает меня здесь и бог знает зачем тогда как ой мог бы получить повышение\n",
      "(observations): юдояюй шпмъочмдроьшм0уо охю1оьдпмъоьп3мчоъю1шпойпйоюдочю1ох2ояюфн3 ъуояюв2гмд м\n"
     ]
    }
   ],
   "source": [
    "plaintext_russian = 'plaintext/russian.txt'\n",
    "ciphertext_russian = 'encrypted/99_encrypted.txt'\n",
    "\n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext_russian)\n",
    "\n",
    "plaintext_sequences = load_sequences(plaintext_russian, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext_russian, char_to_int_mapping)\n",
    "pi_ru, A_ru = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n",
    "\n",
    "\n",
    "\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, max_len=15)\n",
    "\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:50], parameters={'pi': pi_ru, 'A': A_ru}, max_iters=60)\n",
    "# take first 50 encripted examples to reduce time of processing\n",
    "# for the same reason change the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3 Results\n",
    "- Hmm works quite good for that section but we have some artifacts\n",
    "- And also we can guess from that text, that the book is \"War and piece\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task4 Results\n",
    "\n",
    "Already did\n",
    "- Save just outermost for-loop for `forward` and `backward` function. \n",
    "- Optimize compute `xi` for-loop at `forward_backward` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian():\n",
    "    russian_list = []\n",
    "    for i in range(143):\n",
    "        try:\n",
    "            ciphertext_find = 'encrypted/{}_encrypted.txt'.format(i)\n",
    "            encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "            russian_list.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    return russian_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 98,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 104,\n",
       " 106,\n",
       " 109,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 124,\n",
       " 126,\n",
       " 127,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 136,\n",
       " 138,\n",
       " 141,\n",
       " 142]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl = get_russian()\n",
    "rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_find = 'plaintext/english.txt'\n",
    "char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plaintext_find)\n",
    "plaintext_sequences_tt = load_sequences(plaintext_find, char_to_int_mapping_tt)\n",
    "pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ciphertext_find = 'encrypted/26_encrypted.txt' # longer sequences in russian\n",
    "encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "\n",
    "def find_min(files, cipher_file, seq_size=100, iters=60):\n",
    "    scores = []\n",
    "    for plain_file in files:\n",
    "        print(plain_file)\n",
    "        char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plain_file)\n",
    "        plaintext_sequences_tt = load_sequences(plain_file, char_to_int_mapping_tt)\n",
    "        \n",
    "        ciphertext_find = cipher_file # longer sequences in russian\n",
    "        encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "        \n",
    "        pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))\n",
    "        # Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "        state_to_char = int_to_char_mapping_tt\n",
    "        observation_to_char = int_to_char_mapping_tt\n",
    "\n",
    "        # Initialize a HMM with the correct state/output spaces.\n",
    "        hmm_tt = HMM(observation_to_char, state_to_char)\n",
    "\n",
    "        # Estimate the parameters and decode the encrypted sequences.\n",
    "        llh, res = hmm_tt.estimate_with_em(encrypted_sequences_tt[:seq_size],\n",
    "                                           parameters={'pi': pi_tt, 'A': A_tt}, max_iters=iters)\n",
    "        scores.append((llh, res))\n",
    "        clear_output()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shec  142\n",
      "['(states):       the heart of brothers govern in our loves\\n(observations): noqxoqehnxwpxkhwnoqhjxvwzqhixgixwthxdwzqj', '(states):       and sway our great designs\\n(observations): ei xjmebxwthxvhqenx qjgvij', '(states):       ckesar there is my han\\n(observations): ceqjehxnoqhqxgjxfbxoei', '(states):       a sister i bequeath you whom no brother\\n(observations): exjgjnqhxgxkqrtqenoxbwtxmowfxiwxkhwnoqh', '(states):       id iver love so dearly let her live\\n(observations): g xqzqhxdwzqxjwx qehdbxdqnxoqhxdgzq', '(states):       to join our kingdomy and our hearts and never\\n(observations): nwxlwgixwthxagiv wfjxei xwthxoqehnjxei xiqzqh', '(states):       fly off our loves again\\n(observations): pdbxwppxwthxdwzqjxevegi', '(states):       lexidus happily amen\\n(observations): dqyg tjxoeyygdbxefqi', '(states):       antond i did not thind to draw my sword gainst pompey\\n(observations): einwibxgx g xiwnxnogiaxnwx hemxfbxjmwh xvegijnxywfyqb', '(states):       for he hath laid sthange courtesies and great\\n(observations): pwhxoqxoenoxdeg xjnheivqxcwthnqjgqjxei xvhqen']\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'plaintext/dickens.txt',\n",
    "    'plaintext/shakespeare.txt'\n",
    "]\n",
    "\n",
    "rr = []\n",
    "for num in rl:\n",
    "    res = find_min(files, 'encrypted/{}_encrypted.txt'.format(num))\n",
    "    \n",
    "    if res[0][0] > res[1][0]:\n",
    "        print('dick ', num)\n",
    "        print(res[0][1])\n",
    "        rr.append([('dick', num), (res[0][1])])\n",
    "    else:\n",
    "        print('shec ', num)\n",
    "        print(res[1][1])\n",
    "        rr.append([('shec', num), (res[1][1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results', 'wb') as f:\n",
    "    pickle.dump(rr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for rs in rr:\n",
    "#     print(rs[0])\n",
    "    if rs[0][0] == 'shec':\n",
    "        data.append('encrypted/{}_encrypted.txt '.format(rs[0][1]) +  'plaintext/shakespeare.txt\\n')\n",
    "    else:\n",
    "        data.append('encrypted/{}_encrypted.txt '.format(rs[0][1]) +  'plaintext/dickens.txt\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_auth.txt', 'w', encoding='utf8') as f:\n",
    "    f.writelines(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make guess from all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_find = 'plaintext/russian.txt'\n",
    "char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plaintext_find)\n",
    "plaintext_sequences_tt = load_sequences(plaintext_find, char_to_int_mapping_tt)\n",
    "pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciphertext_find = 'encrypted/1_encrypted.txt'\n",
    "encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -15478.0926\n",
      "iteration 10; log likelihood -10040.9136\n",
      "iteration 20; log likelihood -9670.9460\n",
      "iteration 30; log likelihood -9571.8409\n",
      "iteration 40; log likelihood -9519.8634\n",
      "iteration 50; log likelihood -9487.9079\n",
      "iteration 60; log likelihood -9474.0195\n",
      "iteration 70; log likelihood -9466.8627\n",
      "iteration 80; log likelihood -9462.4410\n",
      "iteration 90; log likelihood -9458.6762\n",
      "(states):       настедой и ив y т\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       y bucour pofour с нитьнето\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       cheraverthiomanceratore me eratomuthe\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       e и ожеретречтечести muthig вr \n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       thi ве тn в яe y n  говеласте tomuthe\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       e nd прей wice the\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):        pofourithe hator бретьний bucou\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       y bucoun r y\n",
      "(observations): cejjgtjxgxef\n",
      "(states):        pofoungre arator бahave\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       y bucouromeand еatеy брnчеда here me end uthe\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "state_to_char = int_to_char_mapping_tt\n",
    "observation_to_char = int_to_char_mapping_tt\n",
    "\n",
    "hmm_tt = HMM(observation_to_char, state_to_char, max_len=15)\n",
    "\n",
    "llh, res = hmm_tt.estimate_with_em(encrypted_sequences_tt[:100], parameters={'pi': pi_tt, 'A': A_tt}, max_iters=100)\n",
    "# scores.append((llh, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian():\n",
    "    russian_list = []\n",
    "    for i in range(143):\n",
    "        try:\n",
    "            ciphertext_find = 'encrypted/{}_encrypted.txt'.format(i)\n",
    "            encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "            russian_list.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    return russian_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = get_russian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    'plaintext/evgeniy-onegin.txt',\n",
    "    'plaintext/khlebnikov.txt',\n",
    "    'plaintext/mayakovsky.txt',\n",
    "    'plaintext/voina-i-mir-tom-1.txt',\n",
    "    'plaintext/voina-i-mir-tom-2.txt'\n",
    "]\n",
    "\n",
    "rr = []\n",
    "for num in rl:\n",
    "    res = find_min(files, 'encrypted/{}_encrypted.txt'.format(num), seq_size=100, iters=50)\n",
    "    print(num)\n",
    "    rr.append([(num), (res)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('res_rus.pkl', 'wb') as f:\n",
    "    pickle.dump(rr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 11,\n",
       " 14,\n",
       " 17,\n",
       " 19,\n",
       " 26,\n",
       " 27,\n",
       " 32,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 80,\n",
       " 86,\n",
       " 97,\n",
       " 99,\n",
       " 103,\n",
       " 105,\n",
       " 107,\n",
       " 108,\n",
       " 110,\n",
       " 123,\n",
       " 125,\n",
       " 128,\n",
       " 129,\n",
       " 135,\n",
       " 137,\n",
       " 139,\n",
       " 140]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[el[0] for el in rr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-6984.3410486666135,\n",
       " -6983.513129827719,\n",
       " -6565.6277146375705,\n",
       " -6627.433742312295,\n",
       " -6999.177615027825]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = 5\n",
    "print(rr[nn][0])\n",
    "[s[0] for s in rr[nn][1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 5, 'plaintext/mayakovsky.txt')\n",
      "(1, 11, 'plaintext/voina-i-mir-tom-1.txt')\n",
      "(2, 14, 'plaintext/voina-i-mir-tom-2.txt')\n",
      "(3, 17, 'plaintext/evgeniy-onegin.txt')\n",
      "(4, 19, 'plaintext/voina-i-mir-tom-2.txt')\n",
      "(5, 26, 'plaintext/mayakovsky.txt')\n",
      "(6, 27, 'plaintext/voina-i-mir-tom-1.txt')\n",
      "(7, 32, 'plaintext/mayakovsky.txt')\n",
      "(8, 62, 'plaintext/voina-i-mir-tom-1.txt')\n",
      "(9, 63, 'plaintext/khlebnikov.txt')\n",
      "(10, 64, 'plaintext/voina-i-mir-tom-2.txt')\n",
      "(11, 80, 'plaintext/mayakovsky.txt')\n",
      "(12, 86, 'plaintext/evgeniy-onegin.txt')\n",
      "(13, 97, 'plaintext/khlebnikov.txt')\n",
      "(14, 99, 'plaintext/voina-i-mir-tom-1.txt')\n",
      "(15, 103, 'plaintext/khlebnikov.txt')\n",
      "(16, 105, 'plaintext/mayakovsky.txt')\n",
      "(17, 107, 'plaintext/mayakovsky.txt')\n",
      "(18, 108, 'plaintext/mayakovsky.txt')\n",
      "(19, 110, 'plaintext/mayakovsky.txt')\n",
      "(20, 123, 'plaintext/khlebnikov.txt')\n",
      "(21, 125, 'plaintext/evgeniy-onegin.txt')\n",
      "(22, 128, 'plaintext/khlebnikov.txt')\n",
      "(23, 129, 'plaintext/voina-i-mir-tom-2.txt')\n",
      "(24, 135, 'plaintext/mayakovsky.txt')\n",
      "(25, 137, 'plaintext/mayakovsky.txt')\n",
      "(26, 139, 'plaintext/voina-i-mir-tom-2.txt')\n",
      "(27, 140, 'plaintext/mayakovsky.txt')\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for el in rr:\n",
    "    num = el[0]\n",
    "    ind = np.argmax([s[0] for s in el[1]])\n",
    "    print((i, num, files[ind]))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[63,\n",
       " [(-4888.620693898795,\n",
       "   ['(states):       хлестать\\n(observations): 20ьйм4мф',\n",
       "    '(states):       с бронированных чтидек железа\\n(observations): йпл ко9 къ4ооц2пдм9ньупеь0ь34',\n",
       "    '(states):       и газа\\n(observations): 9пт434',\n",
       "    '(states):       кровавый подет свотри\\n(observations): у къ4ъцяпдк6ьмпй6км 9',\n",
       "    '(states):       выстучает\\n(observations): ъцймвд4ьм',\n",
       "    '(states):       из близких лет костьби постукивает\\n(observations): 93пл093у92п0ьмпукймф69пдкймву9ъ4ьм',\n",
       "    '(states):       лональкрасе на ней\\n(observations): 0кс4жфу 4й4по4поья',\n",
       "    '(states):       войны\\n(observations): ъкяоц',\n",
       "    '(states):       пожечтелый скалет и сталью\\n(observations): дкеь0мь0цяпйуь0ьмп9пйм40фч',\n",
       "    '(states):       синеет\\n(observations): й9оььм']),\n",
       "  (-4887.7919257759995,\n",
       "   ['(states):       хлестать\\n(observations): 20ьйм4мф',\n",
       "    '(states):       с бронированных этищек железа\\n(observations): йпл ко9 къ4ооц2пдм9ньупеь0ь34',\n",
       "    '(states):       и газа\\n(observations): 9пт434',\n",
       "    '(states):       кровавый повет свотри\\n(observations): у къ4ъцяпдк6ьмпй6км 9',\n",
       "    '(states):       выступает\\n(observations): ъцймвд4ьм',\n",
       "    '(states):       из близких лет костьни постукивает\\n(observations): 93пл093у92п0ьмпукймф69пдкймву9ъ4ьм',\n",
       "    '(states):       лошадькраса на ней\\n(observations): 0кс4жфу 4й4по4поья',\n",
       "    '(states):       войны\\n(observations): ъкяоц',\n",
       "    '(states):       пожестелый скалет и сталью\\n(observations): дкеь0мь0цяпйуь0ьмп9пйм40фч',\n",
       "    '(states):       синеет\\n(observations): й9оььм']),\n",
       "  (-5245.286022153759,\n",
       "   ['(states):       и бнемез\\n(observations): 20ьйм4мф',\n",
       "    '(states):       ать ныв нимый ить водетовебо \\n(observations): йпл ко9 къ4ооц2пдм9ньупеь0ь34',\n",
       "    '(states):       студо \\n(observations): 9пт434',\n",
       "    '(states):       а щи и этьнодетноне в\\n(observations): у къ4ъцяпдк6ьмпй6км 9',\n",
       "    '(states):       и нель ве\\n(observations): ъцймвд4ьм',\n",
       "    '(states):       воть соеритедета незость нераси ве\\n(observations): 93пл093у92п0ьмпукймф69пдкймву9ъ4ьм',\n",
       "    '(states):       енудаза да ты точе\\n(observations): 0кс4жфу 4й4по4поья',\n",
       "    '(states):       ищей \\n(observations): ъкяоц',\n",
       "    '(states):       ьнобе бе этрадедетстремезв\\n(observations): дкеь0мь0цяпйуь0ьмп9пйм40фч',\n",
       "    '(states):       ася ве\\n(observations): й9оььм']),\n",
       "  (-5189.814615254721,\n",
       "   ['(states):       ль прора\\n(observations): 20ьйм4мф',\n",
       "    '(states):        сть нодал ннобстрая бый ков \\n(observations): йпл ко9 къ4ооц2пдм9ньупеь0ь34',\n",
       "    '(states):        стов \\n(observations): 9пт434',\n",
       "    '(states):       ль лоли стат вы тарь \\n(observations): у къ4ъцяпдк6ьмпй6км 9',\n",
       "    '(states):       лопрету в\\n(observations): ъцймвд4ьм',\n",
       "    '(states):       овышковлобык вылапратостапрелолу в\\n(observations): 93пл093у92п0ьмпукймф69пдкймву9ъ4ьм',\n",
       "    '(states):       кая маль поскосно \\n(observations): 0кс4жфу 4й4по4поья',\n",
       "    '(states):       ла но\\n(observations): ъкяоц',\n",
       "    '(states):       тай грего спр г вы спругаю\\n(observations): дкеь0мь0цяпйуь0ьмп9пйм40фч',\n",
       "    '(states):       поно в\\n(observations): й9оььм']),\n",
       "  (-5210.511537850262,\n",
       "   ['(states):       пе бы ем\\n(observations): 20ьйм4мф',\n",
       "    '(states):       оной илари пязакое этоть еть \\n(observations): йпл ко9 къ4ооц2пдм9ньупеь0ь34',\n",
       "    '(states):        не ь \\n(observations): 9пт434',\n",
       "    '(states):       ой и и сторутодварой \\n(observations): у къ4ъцяпдк6ьмпй6км 9',\n",
       "    '(states):       а бымо то\\n(observations): ъцймвд4ьм',\n",
       "    '(states):        чтое челанегодо быму ко бымоли то\\n(observations): 93пл093у92п0ьмпукймф69пдкймву9ъ4ьм',\n",
       "    '(states):       ера имой в ня кя с\\n(observations): 0кс4жфу 4й4по4поья',\n",
       "    '(states):       и сяз\\n(observations): ъкяоц',\n",
       "    '(states):       ори во е ство етон двышем \\n(observations): дкеь0мь0цяпйуь0ьмп9пйм40фч',\n",
       "    '(states):       оль то\\n(observations): й9оььм'])]]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(plain_file, cipher_files, out_files, iters=41):\n",
    "    scores = []\n",
    "    \n",
    "    char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plain_file)\n",
    "    plaintext_sequences_tt = load_sequences(plain_file, char_to_int_mapping_tt)\n",
    "    pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))\n",
    "        \n",
    "    for i, cipher_file in enumerate(cipher_files):\n",
    "        print(cipher_file)\n",
    "        ciphertext_find = cipher_file # longer sequences in russian\n",
    "        encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "        \n",
    "        state_to_char = int_to_char_mapping_tt\n",
    "        observation_to_char = int_to_char_mapping_tt\n",
    "\n",
    "        hmm_tt = HMM(observation_to_char, state_to_char, max_len=15)\n",
    "\n",
    "        llh, res = hmm_tt.estimate_with_em(encrypted_sequences_tt[:200],\n",
    "                                           parameters={'pi': pi_tt, 'A': A_tt}, max_iters=iters)\n",
    "        with open(out_files[i], 'w', encoding='utf8') as f:\n",
    "            f.writelines(hmm_tt.output_decripted(encrypted_sequences_tt))\n",
    "        clear_output()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "enc_files = []\n",
    "dec_files = []\n",
    "for num in rl[:5]:\n",
    "    enc_files.append('encrypted/{}_encrypted.txt'.format(num))\n",
    "    dec_files.append('decripted/{}_decrypted.txt'.format(num))\n",
    "    \n",
    "decode_text('plaintext/russian.txt', enc_files, dec_files, iters=100)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
