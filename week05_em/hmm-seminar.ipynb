{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    move_out_count = np.zeros(num_states)\n",
    "\n",
    "    for n, sequence in enumerate(sequences):\n",
    "        for prev, nxt in zip(sequence[:-1], sequence[1:]):\n",
    "            A_counts[prev, nxt] += 1\n",
    "            pi_counts[prev] += 1\n",
    "        if len(sequence) > 0:\n",
    "            pi_counts[sequence[-1]] += 1\n",
    "            \n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, axis=1)[:, None]\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.4 s, sys: 4.25 ms, total: 5.4 s\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}, max_len=15):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping) # number of states = number of chars \n",
    "        self.num_outputs = len(observations_to_char_mapping) # number of outputs ?\n",
    "        self.states_to_char_mapping = states_to_char_mapping # get char from state\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping # what is observations ??? \n",
    "        \n",
    "        self.max_len = max_len ## MAX LEN OF ENCRIPTED TEXT\n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)# init pi with random \n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states) # init transition matrix with random \n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs) # init emission matrix \n",
    "        self.B /= np.sum(self.B, 1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        self.pi = parameters['pi'] if self.fixed_pi else self.pi \n",
    "        \n",
    "        self.fixed_A = 'A' in parameters\n",
    "        self.A = parameters['A'] if self.fixed_A else self.A\n",
    "        \n",
    "        self.fixed_B = 'B' in parameters\n",
    "        self.B = parameters['B'] if self.fixed_B else self.B\n",
    "           \n",
    "        previous_llh = None # prev log-likelihood\n",
    "        \n",
    "        iteration = 0\n",
    "        while True and iteration < max_iters: # why we dont use for-loop here? \n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences) # E-step from EM-algo\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts) # M-step from EM-algo\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            if iteration % 10 == 0:\n",
    "                # clear_output()\n",
    "                print('iteration %d; log likelihood %.4f' % (iteration, log_likelihood))\n",
    "                # self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iteration += 1\n",
    "        return previous_llh, self.output(sequences[:10])\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "\n",
    "                # Run Forward-Backward dynamic program\n",
    "                alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sub_seq)\n",
    "\n",
    "                # Accumulate statistics.\n",
    "                pi_counts += gamma[:, 0]\n",
    "                A_counts += xi\n",
    "                for t, x in enumerate(sub_seq):\n",
    "                    B_counts[:, x] += gamma[:, t]\n",
    "\n",
    "                total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence) \n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        ss = np.sum(alpha * beta, 0)\n",
    "        if (ss == 0).any():\n",
    "            print('err')\n",
    "        assert np.isfinite(ss).all()\n",
    "        gamma = (alpha * beta) / ss\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        if not self.fixed_A:\n",
    "            for t in range(1, len(sequence)-1):\n",
    "                this_xi = np.zeros_like(self.A) ## Changed for better performance \n",
    "                this_xi += (self.A * alpha[:, t][:, None]) * (beta[:, t+1] * self.B[:, sequence[t+1]])      \n",
    "                xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        alpha[:, 0] = self.pi * self.B[:, sequence[0]]\n",
    "        for t in range(len(sequence) - 1):\n",
    "            x = sequence[t+1]\n",
    "            alpha[:, t+1] = self.B[:, x] * np.sum(self.A * alpha[:, t][:, None], axis=0)\n",
    "        return alpha \n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        beta[:, -1] = 1\n",
    "        \n",
    "        for i, x in enumerate(sequence[1:][::-1]):\n",
    "            t = len(sequence) - 1 - i\n",
    "            beta[:, t-1] = np.sum(self.A * (beta[:, t] * self.B[:, x]), axis=1)\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        res = []\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = []\n",
    "            map_states = []\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "                sub_observations = [self.observations_to_char_mapping[x] for x in sub_seq]                \n",
    "                sub_map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sub_seq)]\n",
    "                observations.append(''.join(sub_observations))\n",
    "                map_states.append(''.join(sub_map_states))\n",
    "                \n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "            res.append('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n",
    "        return res\n",
    "    \n",
    "    \n",
    "    def output_decripted(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        res = []\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = []\n",
    "            map_states = []\n",
    "            for i in range(0, len(sequence), self.max_len):\n",
    "                sub_seq = sequence[i:i+self.max_len] \n",
    "                sub_map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sub_seq)]\n",
    "                map_states.append(''.join(sub_map_states))\n",
    "            res.append(''.join(map_states) + '\\n')\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -12081.8386\n",
      "iteration 10; log likelihood -9925.3214\n",
      "iteration 20; log likelihood -9558.5769\n",
      "iteration 30; log likelihood -9474.5125\n",
      "iteration 40; log likelihood -9427.6650\n",
      "iteration 50; log likelihood -9381.4015\n",
      "iteration 60; log likelihood -9340.9561\n",
      "iteration 70; log likelihood -9276.4958\n",
      "iteration 80; log likelihood -9113.3362\n",
      "iteration 90; log likelihood -8902.4124\n",
      "(states):       than such a roman\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       cancous britis bait fot me\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       ill fot endure it you forget yourself\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       to hedge me in i am a soldier i\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       older in prantice abler than yourself\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       to mave conditions\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       britis go to you are fot cassius\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       cancous i am\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       britis i say you are fot\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       cancous urge me fo more i shall forget myself\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, max_len=15)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:100], parameters={'pi': pi, 'A': A})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 results \n",
    "We can see that hmm works good enough with small english texts\n",
    "\n",
    "p.s. this is work of algorithm with task3-4 improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -22976.0006\n",
      "iteration 10; log likelihood -16703.9203\n",
      "iteration 20; log likelihood -15969.2359\n",
      "iteration 30; log likelihood -15550.6683\n",
      "iteration 40; log likelihood -15500.7274\n",
      "iteration 50; log likelihood -15490.3822\n",
      "iteration 60; log likelihood -15487.1750\n",
      "iteration 70; log likelihood -15486.0711\n",
      "iteration 80; log likelihood -15485.4633\n",
      "iteration 90; log likelihood -15485.0901\n",
      "(states):       князь андрей пожал плечами и поморщился как морщатся любители музыки услышав пальшивую ноту обе женщины отрустили друг друга потом опять как бучто боясь опоздать схватили друг друга за руки стали целовать и отрывать руки и потом опять стали целовать друг друга в лицо и совершенно неожиданно для князя андрея обе 194091907 и опять стали целоваться тоже 1940919ла князю андрей было очевидно неловко но для двух женщий казалось так естественно что они плакали казалось они и не предполагали чтобы могло иначе совершиться это свидание\n",
      "(observations): йдрьуопдшщмзояю4пфояфм3пч о ояючющ8 ф0ройпйочющ8пъ0рофцх ъмф очнь2й он0ф2гпвоипфуг внцодюъноюхмо4мд8 д2оюъян0ъ ф ошщн1ошщн1пояюъючоюяръуойпйохншъюохюр0уоюяюьшпъуо0твпъ ф ошщн1ошщн1поьпощнй о0ъпф о5мфювпъуо оюъщ2впъуощнй о ояюъючоюяръуо0ъпф о5мфювпъуошщн1ошщн1повоф 5юо о0ювмщгмддюодмю4 шпддюошфройдрьропдшщмроюхмоьпяфпйпф о оюяръуо0ъпф о5мфювпъу0роъю4моьпяфпйпфпойдрьцопдшщмцох2фюою3мв шдюодмфювйюодюошфрошвнто4мд8 дойпьпфю0уоъпйом0ъм0ъвмддюо3ъюоюд ояфпйпф ойпьпфю0уоюд о одмоящмшяюфп1пф о3ъюх2очю1фюо дп3мо0ювмщг ъу0ро9ъюо0в шпд м\n",
      "(states):       вдруг заговорили обе женщины и засмеялись с ах милая ах мари а я видела во сне так вы нас не ожидали ах мари вы так похудели а вы так пополнел\n",
      "(observations): вшщн1оьп1ювющ ф оюхмо4мд8 д2о оьп0чмрф 0уо0опточ фпропточпщ опоров шмфповюо0дмоъпйов2одп0одмою4 шпф опточпщ ов2оъпйояютншмф опов2оъпйояюяюфдмф\n",
      "(states):       я тотчас узнала княщиню вставила бурьен\n",
      "(observations): роъюъ3п0оньдпфпойдр1 дцов0ъпв фпохнщумд\n",
      "(states):       восклицала княжна марья а я не подозревала ах я и не видела тебя\n",
      "(observations): вю0йф 5пфпойдр4дпочпщуропородмояюшюьщмвпфпопторо одмов шмфпоъмхр\n",
      "(states):       князь андрей пореловался с сестрой рука в руку и сказал ей что она такая же 409126 как всегда была княжна марья повернулась к брату и сквозь слезы любовный теплый и кроткий взгляд ее прекрасных в ту минуту больших лучистых глаз остановился на лице князя андрея\n",
      "(observations): йдрьуопдшщмзояю5мфювпф0ро0о0м0ъщюцощнйповощнйно о0йпьпфомзо3ъюоюдпоъпйпро4мояфпй0пойпйов0м1шпох2фпойдр4дпочпщурояювмщднфп0уойохщпъно о0йвюьуо0фмь2офцхювд2зоъмяф2зо ойщюъй зовь1фршоммоящмйщп0д2товоъноч днънохюфуг тофн3 0ъ2то1фпьою0ъпдюв ф0родпоф 5мойдрьропдшщмр\n",
      "(states):       княщиня говорила без умолку короткая верхняя губка с усиками то и дело на мгновение слетала вниз притрогивалась где нужно было к румяной нижней губке и вновь открывалась блестевшая зубами и глазами улыбка княщиня 192219191908 случай который был с ними на спасской горе грозивший ей опасностий в че положении и сейчас же после этого стобщила что она все платья свои оставила в петербующе и здесь будет ходить бог знает в чем и что андрей совсем переменился и что кичти одынцова вышла замуж за старика и что есть жених для княжны марьи вполне серьезный но что об этом поговорим после княжна марья все еще молча смотрела на брата и в прекрасных глазах че была и любовь и грусть видно было что в ней установился теперь свой ход мысли независимый от речей невестки она в середине че рассказа о последнем празднике в петербующе обратилась к брату\n",
      "(observations): йдр1 дро1ювющ фпохмьончюфйнойющюъйпровмщтдрро1нхйпо0он0 йпч оъюо ошмфюодпоч1дювмд мо0фмъпфповд ьоящ ъщю1 впфп0уо1шмодн4дюох2фюойощнчрдюзод 4дмзо1нхймо овдювуоюъйщ2впфп0уохфм0ъмвгпроьнхпч о о1фпьпч онф2хйпойдр1 дрощп00йпь2впфпо0фн3пзойюъющ2зох2фо0од ч одпо0яп00йюзо1ющмо1щюь вг зомзоюяп0дю0ъ цовоммояюфю4мд  о о0мз3п0о4мояю0фмо9ъю1юо0ююх8 фпо3ъюоюдпов0мояфпъуро0вю ою0ъпв фповоямъмщхнщ1мо оьшм0уохншмъотюш ъуохю1оьдпмъово3мчо о3ъюопдшщмзо0юв0мчоямщмчмд ф0ро о3ъюой ъъ оюш2д5ювпов2гфпоьпчн4оьпо0ъпщ йпо о3ъюом0ъуо4мд тошфройдр4д2очпщу овяюфдмо0мщумьд2зодюо3ъюоюхо9ъючояю1ювющ чояю0фмойдр4дпочпщуров0мом8мочюф3по0чюъщмфподпохщпъпо овоящмйщп0д2то1фпьптоммох2фпо офцхювуо о1щн0ъуов шдюох2фюо3ъюоводмзон0ъпдюв ф0роъмямщуо0вюзотюшоч20ф одмьпв 0 ч2зоюъощм3мзодмвм0ъй оюдпово0мщмш дмоммощп00йпьпоюояю0фмшдмчоящпьшд ймовоямъмщхнщ1моюхщпъ фп0уойохщпън\n",
      "(states):        ты решительно едешь на войну сказала вздохнув\n",
      "(observations): оъ2ощмг ъмфудюомшмгуодповюздно0йпьпфповьшютднв\n",
      "(states):       вздрогнула тоже\n",
      "(observations): вьшщю1днфпоъю4м\n",
      "(states):       даже завтра отвечал брат\n",
      "(observations): шп4моьпвъщпоюъвм3пфохщпъ\n",
      "(states):       ой покидает меня здесь и бог знает зачем тогда как ой мог бы получить повышение\n",
      "(observations): юдояюй шпмъочмдроьшм0уо охю1оьдпмъоьп3мчоъю1шпойпйоюдочю1ох2ояюфн3 ъуояюв2гмд м\n"
     ]
    }
   ],
   "source": [
    "plaintext_russian = 'plaintext/russian.txt'\n",
    "ciphertext_russian = 'encrypted/99_encrypted.txt'\n",
    "\n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext_russian)\n",
    "\n",
    "plaintext_sequences = load_sequences(plaintext_russian, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext_russian, char_to_int_mapping)\n",
    "pi_ru, A_ru = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))\n",
    "\n",
    "\n",
    "\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, max_len=30)\n",
    "\n",
    "llh, output = hmm.estimate_with_em(encrypted_sequences[:30], parameters={'pi': pi_ru, 'A': A_ru}, max_iters=100)\n",
    "# take first 50 encripted examples to reduce time of processing\n",
    "# for the same reason change the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task3 Results\n",
    "- Hmm works quite good for that section but we have some artifacts\n",
    "- And also we can guess from that text, that the book is \"War and piece\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task4 Results\n",
    "\n",
    "Already did\n",
    "- Save just outermost for-loop for `forward` and `backward` function. \n",
    "- Optimize compute `xi` for-loop at `forward_backward` function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "For each in texts russian and texts in english try to decode file. \n",
    "\n",
    "For each decoded files find original, that best matches (most lines are matched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_russian():\n",
    "    russian_list = []\n",
    "    for i in range(143):\n",
    "        try:\n",
    "            ciphertext_find = 'encrypted/{}_encrypted.txt'.format(i)\n",
    "            encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "            russian_list.append(i)\n",
    "        except:\n",
    "            continue\n",
    "    return russian_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = get_russian()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('res_rus.pkl', 'wb') as f:\n",
    "    pickle.dump(rr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_text(plain_file, cipher_files, out_files, iters=41, max_len=15):\n",
    "    scores = []\n",
    "    \n",
    "    char_to_int_mapping_tt, int_to_char_mapping_tt = get_char_to_int_mapping(plain_file)\n",
    "    plaintext_sequences_tt = load_sequences(plain_file, char_to_int_mapping_tt)\n",
    "    pi_tt, A_tt = estimate_markov_model_from_sequences(plaintext_sequences_tt, len(char_to_int_mapping_tt))\n",
    "        \n",
    "    for i, cipher_file in enumerate(cipher_files):\n",
    "        print(cipher_file)\n",
    "        ciphertext_find = cipher_file # longer sequences in russian\n",
    "        encrypted_sequences_tt = load_sequences(ciphertext_find, char_to_int_mapping_tt)\n",
    "        \n",
    "        state_to_char = int_to_char_mapping_tt\n",
    "        observation_to_char = int_to_char_mapping_tt\n",
    "\n",
    "        hmm_tt = HMM(observation_to_char, state_to_char, max_len=max_len)\n",
    "\n",
    "        llh, res = hmm_tt.estimate_with_em(encrypted_sequences_tt[:200],\n",
    "                                           parameters={'pi': pi_tt, 'A': A_tt}, max_iters=iters)\n",
    "        with open(out_files[i], 'w', encoding='utf8') as f:\n",
    "            f.writelines(hmm_tt.output_decripted(encrypted_sequences_tt))\n",
    "        clear_output()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = [i for i in range(143) if i not in rl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "rr = []\n",
    "enc_files = []\n",
    "dec_files = []\n",
    "for num in rl:\n",
    "    enc_files.append('encrypted/{}_encrypted.txt'.format(num))\n",
    "    dec_files.append('decripted2/{}_decrypted.txt'.format(num))\n",
    "    \n",
    "decode_text('plaintext/russian.txt', enc_files, dec_files, iters=100)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    'plaintext/evgeniy-onegin.txt': {},\n",
    "    'plaintext/khlebnikov.txt': {},\n",
    "    'plaintext/mayakovsky.txt': {},\n",
    "    'plaintext/voina-i-mir-tom-1.txt': {},\n",
    "    'plaintext/voina-i-mir-tom-2.txt': {}\n",
    "}\n",
    "\n",
    "for author in res.keys():\n",
    "    with open(author, 'r') as f:\n",
    "        auth_file = f.read()\n",
    "        \n",
    "    for dec_file in dec_files:\n",
    "        print(dec_file)\n",
    "        with open(dec_file, 'r') as f1:\n",
    "            dec_lines = f1.readlines()\n",
    "        counts = 0 \n",
    "        for line in dec_lines:\n",
    "            if line in auth_file:\n",
    "                counts += 1\n",
    "        if counts > 0:\n",
    "            res[author][dec_file] = counts\n",
    "        \n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_ru = {}\n",
    "i = 0\n",
    "for auth in res.keys():\n",
    "    for dec_file in res[auth].keys():\n",
    "        if dec_file not in res_ru:\n",
    "            res_ru[dec_file] = [0,0,0,0,0,0,0]\n",
    "        res_ru[dec_file][i] = res[auth][dec_file]\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decripted2/103_decrypted.txt': [720, 0, 0, 0, 1, 0, 0],\n",
       " 'decripted2/105_decrypted.txt': [33, 505, 51, 25, 30, 0, 0],\n",
       " 'decripted2/107_decrypted.txt': [14, 17, 17, 12, 11, 0, 0],\n",
       " 'decripted2/108_decrypted.txt': [11, 14, 20, 10, 9, 0, 0],\n",
       " 'decripted2/110_decrypted.txt': [5, 5, 7, 9, 432, 0, 0],\n",
       " 'decripted2/11_decrypted.txt': [59, 64, 566, 52, 46, 0, 0],\n",
       " 'decripted2/123_decrypted.txt': [33, 36, 34, 33, 33, 0, 0],\n",
       " 'decripted2/128_decrypted.txt': [14, 15, 30, 12, 11, 0, 0],\n",
       " 'decripted2/129_decrypted.txt': [56, 65, 594, 54, 53, 0, 0],\n",
       " 'decripted2/135_decrypted.txt': [3, 4, 4, 396, 6, 0, 0],\n",
       " 'decripted2/137_decrypted.txt': [4, 4, 4, 4, 4, 0, 0],\n",
       " 'decripted2/139_decrypted.txt': [0, 1, 1, 0, 0, 0, 0],\n",
       " 'decripted2/140_decrypted.txt': [62, 56, 643, 45, 54, 0, 0],\n",
       " 'decripted2/14_decrypted.txt': [38, 699, 32, 27, 26, 0, 0],\n",
       " 'decripted2/17_decrypted.txt': [737, 1, 1, 1, 1, 0, 0],\n",
       " 'decripted2/19_decrypted.txt': [2, 2, 3, 6, 533, 0, 0],\n",
       " 'decripted2/26_decrypted.txt': [12, 19, 15, 10, 11, 0, 0],\n",
       " 'decripted2/32_decrypted.txt': [21, 534, 21, 20, 17, 0, 0],\n",
       " 'decripted2/5_decrypted.txt': [60, 56, 652, 47, 45, 0, 0],\n",
       " 'decripted2/62_decrypted.txt': [1, 1, 1, 1, 0, 0, 0],\n",
       " 'decripted2/63_decrypted.txt': [21, 36, 39, 29, 26, 0, 0],\n",
       " 'decripted2/64_decrypted.txt': [0, 794, 2, 0, 0, 0, 0],\n",
       " 'decripted2/80_decrypted.txt': [13, 26, 21, 8, 7, 0, 0],\n",
       " 'decripted2/86_decrypted.txt': [2, 3, 2, 2, 1, 0, 0],\n",
       " 'decripted2/97_decrypted.txt': [1, 4, 2, 1, 1, 0, 0],\n",
       " 'decripted2/99_decrypted.txt': [1, 3, 1, 1, 1, 0, 0]}"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ru_res', 'wb') as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_enc_map = {dec:enc for dec, enc in  zip(dec_files, enc_files)} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'plaintext/evgeniy-onegin.txt',\n",
    "    'plaintext/khlebnikov.txt',\n",
    "    'plaintext/mayakovsky.txt',\n",
    "    'plaintext/voina-i-mir-tom-1.txt',\n",
    "    'plaintext/voina-i-mir-tom-2.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for dec in dec_files:\n",
    "    if dec not in res_ru:\n",
    "        out.append('{} -------\\n'.format(dec_enc_map[dec]))\n",
    "        continue\n",
    "    \n",
    "    i = np.argmax(res_ru[dec])\n",
    "    \n",
    "    if res_ru[dec][i] > 50:\n",
    "        out.append('{} {}\\n'.format(dec_enc_map[dec], files[i]))\n",
    "    else:\n",
    "        out.append('{} -------\\n'.format(dec_enc_map[dec]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['encrypted/5_encrypted.txt plaintext/mayakovsky.txt\\n',\n",
       " 'encrypted/11_encrypted.txt plaintext/mayakovsky.txt\\n',\n",
       " 'encrypted/14_encrypted.txt plaintext/khlebnikov.txt\\n',\n",
       " 'encrypted/17_encrypted.txt plaintext/evgeniy-onegin.txt\\n',\n",
       " 'encrypted/19_encrypted.txt plaintext/voina-i-mir-tom-2.txt\\n',\n",
       " 'encrypted/26_encrypted.txt -------\\n',\n",
       " 'encrypted/27_encrypted.txt -------\\n',\n",
       " 'encrypted/32_encrypted.txt plaintext/khlebnikov.txt\\n',\n",
       " 'encrypted/62_encrypted.txt -------\\n',\n",
       " 'encrypted/63_encrypted.txt -------\\n',\n",
       " 'encrypted/64_encrypted.txt plaintext/khlebnikov.txt\\n',\n",
       " 'encrypted/80_encrypted.txt -------\\n',\n",
       " 'encrypted/86_encrypted.txt -------\\n',\n",
       " 'encrypted/97_encrypted.txt -------\\n',\n",
       " 'encrypted/99_encrypted.txt -------\\n',\n",
       " 'encrypted/103_encrypted.txt plaintext/evgeniy-onegin.txt\\n',\n",
       " 'encrypted/105_encrypted.txt plaintext/khlebnikov.txt\\n',\n",
       " 'encrypted/107_encrypted.txt -------\\n',\n",
       " 'encrypted/108_encrypted.txt -------\\n',\n",
       " 'encrypted/110_encrypted.txt plaintext/voina-i-mir-tom-2.txt\\n',\n",
       " 'encrypted/123_encrypted.txt -------\\n',\n",
       " 'encrypted/125_encrypted.txt -------\\n',\n",
       " 'encrypted/128_encrypted.txt -------\\n',\n",
       " 'encrypted/129_encrypted.txt plaintext/mayakovsky.txt\\n',\n",
       " 'encrypted/135_encrypted.txt plaintext/voina-i-mir-tom-1.txt\\n',\n",
       " 'encrypted/137_encrypted.txt -------\\n',\n",
       " 'encrypted/139_encrypted.txt -------\\n',\n",
       " 'encrypted/140_encrypted.txt plaintext/mayakovsky.txt\\n']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result_ru.txt', 'w') as f:\n",
    "    f.writelines(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
